{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Grader with Gemini Pro\n",
    "This notebook can grade students’ assignments automatically by downloading them from Moodle LMS. It will unzip the assignment file from Moodle and create a folder for each student. If a student submits a zip file, it will also unzip it in their folder. The folder should contain either some Docx files or one PDF file. For Docx files, the notebook will extract and merge all the texts into one answer. For PDF files, it will only extract the text from the first page as the answer.\n",
    "\n",
    "The notebook will then use a marking scheme as prompts and let Gemini Pro evaluate the answer according to the rules. It will also estimate the probability that the answer is copied from the internet or generated by AI.\n",
    "\n",
    "The notebook will use textembedding-gecko-multilingual@001 to get the embedding of the answer. It will then use K-means clustering to group the answers based on their embeddings and show the teachers the different types of answers. It will also perform PCA on the embeddings and plot the first three principal components in 3D. This will help the teachers see how similar or different the answers are."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q pypandoc docx2txt PyPDF2 openpyxl python-dotenv google-cloud-aiplatform google-cloud-core num2words matplotlib plotly scipy scikit-learn pandas tiktoken ipywidgets seaborn ipympl"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read text file and return the content\n",
    "def read_text_file(path):\n",
    "    with open(path, 'r') as file:\n",
    "        data = file.read().replace('\\n', '')\n",
    "    return data\n",
    "\n",
    "def write_text_to_file(path, content):\n",
    "    with open(path, 'w') as file:\n",
    "        file.write(content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract all submissions to a tmp folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the zipfile module\n",
    "from zipfile import ZipFile\n",
    "# Create a zip file object using ZipFile class\n",
    "with ZipFile(\"data/submission.zip\", \"r\") as zip_obj:\n",
    "    # Extract all the files into a directory\n",
    "    zip_obj.extractall(\"tmp/submission\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the os module\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the path to list\n",
    "temp_path = \"tmp/submission/\"\n",
    "\n",
    "def is_folder_contains_file(folder_path, extension): \n",
    "    # Get a list of all files and directories in the path \n",
    "    names = os.listdir(folder_path) \n",
    "    for name in names: \n",
    "        if name.endswith(extension): \n",
    "            return True \n",
    "    return False   \n",
    "    \n",
    "# Get a list of all files and directories in the path\n",
    "def get_submissions_df(path):\n",
    "    assignment_folders = []\n",
    "    names = os.listdir(path)\n",
    "    # Loop through the list\n",
    "    for name in names:\n",
    "        # Join the path and the name\n",
    "        full_path = os.path.join(path, name)\n",
    "        # Check if it is a directory\n",
    "        if os.path.isdir(full_path):\n",
    "            # Print the directory name\n",
    "            assignment_folders.append({\n",
    "                \"Student\": name.split(\"_\")[0],\n",
    "                \"Path\": full_path,\n",
    "                \"ContainsDocxFile\": is_folder_contains_file(full_path, \".docx\"),                \n",
    "                \"ContainsPdfFile\": is_folder_contains_file(full_path, \".pdf\"),\n",
    "                \"ContainsZipFile\": is_folder_contains_file(full_path, \".zip\")\n",
    "                })\n",
    "    df = pd.DataFrame([p for p in assignment_folders])\n",
    "    return df\n",
    "df = get_submissions_df(temp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensure that all the files submitted are valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_df_by_not_contains_any_expected_files(df):\n",
    "    return df[(df[\"ContainsDocxFile\"] == False) & (df[\"ContainsPdfFile\"] == False) & (df[\"ContainsZipFile\"] == False)]\n",
    "filter_df_by_not_contains_any_expected_files(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handle zip file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def flatten(directory):\n",
    "    for dirpath, _, filenames in os.walk(directory, topdown=False):\n",
    "        for filename in filenames:\n",
    "            i = 0\n",
    "            source = os.path.join(dirpath, filename)\n",
    "            target = os.path.join(directory, filename)\n",
    "\n",
    "            while os.path.exists(target):\n",
    "                i += 1\n",
    "                file_parts = os.path.splitext(os.path.basename(filename))\n",
    "\n",
    "                target = os.path.join(\n",
    "                    directory,\n",
    "                    file_parts[0] + \"_\" + str(i) + file_parts[1],\n",
    "                )\n",
    "\n",
    "            shutil.move(source, target)\n",
    "\n",
    "            print(\"Moved \", source, \" to \", target)\n",
    "\n",
    "        if dirpath != directory:\n",
    "            os.rmdir(dirpath)\n",
    "            print(\"Deleted \", dirpath)\n",
    "\n",
    "def get_first_file_path(path, ext):\n",
    "    names = os.listdir(path)\n",
    "    for name in names:\n",
    "        if name.endswith(ext):\n",
    "            return os.path.join(path, name)\n",
    "\n",
    "def extract_zip_file_in_place(path):\n",
    "    zip_path = get_first_file_path(path, \".zip\")\n",
    "    print(zip_path)\n",
    "    import zipfile\n",
    "    # Create a zip file object using ZipFile class\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zip_obj:\n",
    "        # Extract all the files into a directory\n",
    "        zip_obj.extractall(path)\n",
    "    flatten(path) \n",
    "\n",
    "\n",
    "def filter_df_by_contains_zip_file(df):\n",
    "    return df[(df[\"ContainsZipFile\"] == True)]\n",
    "\n",
    "paths = filter_df_by_contains_zip_file(df)[\"Path\"].values\n",
    "for path in paths:\n",
    "    extract_zip_file_in_place(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_submissions_df(temp_path)\n",
    "## check all rows contains Docx or PDF file\n",
    "def filter_df_by_contains_docx_or_pdf_file(df):\n",
    "    return df[(df[\"ContainsDocxFile\"] == True) | (df[\"ContainsPdfFile\"] == True)]\n",
    "\n",
    "filter_df_by_contains_docx_or_pdf_file(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Docx files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_df_by_contains_docx(df):\n",
    "    return df[(df[\"ContainsDocxFile\"] == True)]\n",
    "words_df = filter_df_by_contains_docx(df)\n",
    "paths = words_df[\"Path\"].values\n",
    "\n",
    "def get_all_docx_files(path):\n",
    "    import glob\n",
    "    return glob.glob(path + \"/*.docx\")\n",
    "\n",
    "import docx2txt\n",
    "from functools import reduce\n",
    "\n",
    "students_words_files = list(map(get_all_docx_files, paths)) # List of lists of word files\n",
    "\n",
    "file_contents =[];\n",
    "for word_files in students_words_files:  \n",
    "    file_contents.append(reduce(lambda x, y: x + y, map(lambda f: docx2txt.process(f), word_files), \"\\n\\n\"))\n",
    "# reduce(map(lambda f: docx2txt.process(f), word_files), lambda x, y: x + y, \"\")\n",
    "words_df.loc[:, \"Sources\"] = students_words_files\n",
    "words_df.loc[:, \"Answers\"] = file_contents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_df_by_contains_pdf(df):\n",
    "    return df[(df[\"ContainsPdfFile\"] == True)]\n",
    "pdfs_df = filter_df_by_contains_pdf(df)\n",
    "paths = pdfs_df[\"Path\"].values\n",
    "\n",
    "def get_add_pdf_files(path):\n",
    "    import glob\n",
    "    return glob.glob(path + \"/*.pdf\")\n",
    "\n",
    "import PyPDF2\n",
    "from functools import reduce\n",
    "\n",
    "def convert_pdf_all_pages_to_txt(path):\n",
    "    pdfFileObj = open(path, 'rb')\n",
    "    reader = PyPDF2.PdfReader(pdfFileObj)\n",
    "    num_pages = len(reader.pages)\n",
    "    count = 0\n",
    "    text = \"\"\n",
    "    while count < num_pages:\n",
    "        pageObj = reader.pages[count]\n",
    "        count += 1\n",
    "        text += pageObj.extract_text()\n",
    "        text += \"\\n\\n\"\n",
    "    return text\n",
    "\n",
    "students_pdf_files = list(map(get_add_pdf_files, paths)) # List of lists of word files\n",
    "\n",
    "file_contents =[];\n",
    "for pdf_files in students_pdf_files:\n",
    "    file_contents.append(reduce(lambda x, y: x + y, map(convert_pdf_all_pages_to_txt, pdf_files), \"\\n\\n\"))\n",
    "\n",
    "pdfs_df.loc[:, \"Sources\"] = students_pdf_files\n",
    "pdfs_df.loc[:, \"Answers\"] = file_contents\n",
    "pdfs_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine two dataframes into one and export to excel\n",
    "df_answers = pd.concat([words_df, pdfs_df])\n",
    "df_answers.to_excel(\"data/answers.xlsx\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grading students’ responses using Gemini Pro with Grounding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = 'cyrus-testing-2023'\n",
    "!gcloud config set project {project_id}\n",
    "!gcloud auth application-default set-quota-project {project_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(\n",
    "    # your Google Cloud Project ID or number\n",
    "    # environment default used is not set\n",
    "    project=project_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import vertexai\n",
    "from vertexai.generative_models import GenerativeModel\n",
    "import vertexai.preview.generative_models as generative_models\n",
    "\n",
    "vertexai.init(project=project_id, location=\"us-central1\")\n",
    "model = GenerativeModel(\"gemini-1.0-pro-001\")\n",
    "\n",
    "def get_json_gemini(student, prompt):        \n",
    "    generation_config = {\n",
    "        \"max_output_tokens\": 4096,\n",
    "        \"temperature\": 0.2,\n",
    "        \"top_p\": 0.2,\n",
    "    }\n",
    "    safety_settings = {\n",
    "        generative_models.HarmCategory.HARM_CATEGORY_HATE_SPEECH: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "        generative_models.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "        generative_models.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "        generative_models.HarmCategory.HARM_CATEGORY_HARASSMENT: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "    }   \n",
    "  \n",
    "    model_response  = model.generate_content(\n",
    "        [prompt],\n",
    "        generation_config=generation_config,\n",
    "        safety_settings=safety_settings\n",
    "    )\n",
    "    text = model_response.candidates[0].content.parts[0].text\n",
    "    print(text)\n",
    "\n",
    "    write_text_to_file(f\"tmp/{student}.json\", json.dumps(text))\n",
    "    tokens = model_response.usage_metadata.total_token_count\n",
    "    \n",
    "    return json.loads(text) , tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grade_answer(student,student_answer, marking_scheme):    \n",
    "    prompt=marking_scheme.replace(\"<ANSWER></ANSWER>\", student_answer)\n",
    "    retry = 0; \n",
    "    while True:\n",
    "        try:\n",
    "            content, tokens = get_json_gemini(student,prompt)\n",
    "            break             \n",
    "        except Exception as e:            \n",
    "            if retry < 2:                \n",
    "                retry += 1\n",
    "                print(e)\n",
    "                print(\"retry: \" + str(retry))\n",
    "                continue            \n",
    "            return 0, \"Error\", 0, 0, True, 0, True\n",
    "    marks = content['marks']\n",
    "    comments = content['comments']       \n",
    "    copyFromInternet = content['copyFromInternet']\n",
    "    generativeAI = content['generativeAI']        \n",
    "    manualReview = content['manualReview']     \n",
    "    return marks, comments, copyFromInternet, generativeAI, manualReview, tokens, False    \n",
    "\n",
    "def grade_answers(df_answers, marking_scheme):  \n",
    "    for index, row in df_answers.iterrows():              \n",
    "        student = row[\"Student\"]\n",
    "        print(student)\n",
    "        answer = row[\"Answers\"]       \n",
    "        marks, comments, copyFromInternet, generativeAI, manualReview, tokens, error = grade_answer(student, answer, marking_scheme)\n",
    "        df_answers.loc[index, \"Marks\"] = marks\n",
    "        df_answers.loc[index, \"Comments\"] = comments\n",
    "        df_answers.loc[index, \"CopyFromInternet\"] = copyFromInternet\n",
    "        df_answers.loc[index, \"GenerativeAI\"] = generativeAI\n",
    "        df_answers.loc[index, \"ChatGptTokens\"] = tokens     \n",
    "        df_answers.loc[index, \"ManualReview\"] = manualReview\n",
    "        df_answers.loc[index, \"Error\"] = error       \n",
    "    return df_answers\n",
    "\n",
    "marking_scheme = read_text_file(\"marking_scheme.txt\")\n",
    "\n",
    "# get second row answer for df_answers\n",
    "# student = df_answers.iloc[[2]][\"Student\"].values[0]\n",
    "# student_answer = df_answers.iloc[[2]][\"Answers\"].values[0]\n",
    "# print(student_answer)\n",
    "# grade_answer(student_answer, marking_scheme)\n",
    "\n",
    "df_marked = grade_answers(df_answers, marking_scheme)\n",
    "df_marked.to_excel(\"data/marks.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_marked"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings and clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_marked = pd.read_excel(\"data/marks.xlsx\") \n",
    "df_Answers = df_marked[['Student','Answers']]\n",
    "df_Answers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data cleaning by removing redundant whitespace and cleaning up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "pd.options.mode.chained_assignment = None #https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#evaluation-order-matters\n",
    "\n",
    "# s is input text\n",
    "def normalize_text(s, sep_token = \" \\n \"):\n",
    "    s = re.sub(r'\\s+',  ' ', s).strip()\n",
    "    s = re.sub(r\". ,\",\"\",s)\n",
    "    # remove all instances of multiple spaces\n",
    "    s = s.replace(\"..\",\".\")\n",
    "    s = s.replace(\". .\",\".\")\n",
    "    s = s.replace(\"\\n\", \"\")\n",
    "    s = s.strip()    \n",
    "    return s if len(s) > 0 else \"Do nothing\"\n",
    "\n",
    "df_Answers['Answers']= df_Answers[\"Answers\"].apply(lambda x : normalize_text(x))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove any answers that are too long for the token limit (8192 tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "df_Answers['n_tokens'] = df_Answers[\"Answers\"].apply(lambda x: len(tokenizer.encode(x)))\n",
    "df_Answers = df_Answers[df_Answers.n_tokens<8192]\n",
    "len(df_Answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Answers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import  Optional, List\n",
    "from vertexai.language_models import TextEmbeddingInput, TextEmbeddingModel\n",
    "\n",
    "def get_embedding(\n",
    "    text: str,\n",
    "    task: str = \"CLUSTERING\",\n",
    "    model_name: str = \"text-multilingual-embedding-preview-0409\",\n",
    "    dimensionality: Optional[int] = 256,\n",
    ") -> List[float]:\n",
    "    \"\"\"Embeds texts with a pre-trained, foundational model.\"\"\"\n",
    "    model = TextEmbeddingModel.from_pretrained(model_name)\n",
    "    inputs = [TextEmbeddingInput(text, task)]\n",
    "    kwargs = dict(output_dimensionality=dimensionality) if dimensionality else {}\n",
    "    embeddings = model.get_embeddings(inputs, **kwargs)\n",
    "    return [embedding.values for embedding in embeddings][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Answers['embedding'] = df_Answers[\"Answers\"].apply(lambda x : get_embedding(x)) \n",
    "# df_Answers.set_index( ['Student'], inplace = True)\n",
    "# engine should be set to the deployment name you chose when you deployed the text-embedding-ada-002 (Version 2) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "df_Answers.to_excel(\"data/embeddings.xlsx\", index=True)\n",
    "df_Answers.apply(lambda x : write_text_to_file(f\"tmp/embeddings_{x.Student}.json\", json.dumps(x.embedding)), axis=1)\n",
    "df_Answers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering based on the Embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reload embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# df_embeddings = df_Answers.copy()\n",
    "df_embeddings = pd.read_excel(\"data/embeddings.xlsx\") \n",
    "def reload_embeddings(student):\n",
    "    return list(json.loads(read_text_file(f\"tmp/embeddings_{student.Student}.json\")))\n",
    "df_embeddings[\"embedding\"] = df_embeddings.apply(lambda s : reload_embeddings(s), axis=1)\n",
    "df_embeddings.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "df_embeddings.set_index( ['Student'], inplace = True)\n",
    "df_embeddings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "matrix = np.array(df_embeddings[\"embedding\"].to_list())\n",
    "n_clusters = 7\n",
    "kmeans = KMeans(n_clusters=n_clusters, init=\"k-means++\", random_state=42, n_init='auto')\n",
    "kmeans.fit(matrix)\n",
    "labels = kmeans.labels_\n",
    "df_embeddings[\"Cluster\"] = labels \n",
    "df_embeddings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (15, 8) \n",
    "\n",
    "tsne = TSNE(n_components=2, perplexity=5, random_state=42, init='random', learning_rate=200)\n",
    "vis_dims2 = tsne.fit_transform(matrix)\n",
    "\n",
    "x = [x for x,y in vis_dims2]\n",
    "y = [y for x,y in vis_dims2]\n",
    "\n",
    "palette = sns.color_palette(\"inferno\", 20).as_hex() \n",
    "\n",
    "for category, color in enumerate(palette):    \n",
    "    xs = np.array(x)[df_embeddings[\"Cluster\"]==category]\n",
    "    ys = np.array(y)[df_embeddings[\"Cluster\"]==category]\n",
    "    plt.scatter(xs, ys, color=color, alpha=0.1)\n",
    "\n",
    "    avg_x = xs.mean()\n",
    "    avg_y = ys.mean()\n",
    "    \n",
    "    plt.scatter(avg_x, avg_y, marker='x', color=color, s=100)\n",
    "plt.title(\"Embeddings visualized using t-SNE\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export the final result cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_marked_tmp=pd.read_excel(\"data/marks.xlsx\") \n",
    "df_embeddings_tmp=df_embeddings.copy()\n",
    "df_marked_tmp.set_index( ['Student'], inplace = True)\n",
    "# df_embeddings_tmp.set_index( ['Student'], inplace = True)\n",
    "df_final = pd.merge(df_marked_tmp, df_embeddings_tmp[[\"n_tokens\",\"embedding\",\"Cluster\"]], how='left', left_index=True, right_index=True)\n",
    "\n",
    "cols = ['Marks', 'Comments', 'Answers','CopyFromInternet','GenerativeAI','ChatGptTokens','ManualReview','Error','Cluster']\n",
    "\n",
    "df_final= df_final[cols]\n",
    "df_final.to_excel(\"data/final.xlsx\", index=True)\n",
    "df_final.head(5)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce the embedding dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca_df = df_embeddings.copy()\n",
    "matrix = pca_df[\"embedding\"].to_list()\n",
    "pca = PCA(n_components=3)\n",
    "vis_dims = pca.fit_transform(matrix)\n",
    "pca_df[\"embed_vis\"] = vis_dims.tolist()\n",
    "pca_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ratio of the total variance each principal component captures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(sum(pca.explained_variance_ratio_)*100)+\"%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzing the Change in Explained Variance Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "nums = np.arange(14)\n",
    "\n",
    "var_ratio = []\n",
    "for num in nums:\n",
    "  pca = PCA(n_components=num)\n",
    "  pca.fit(matrix)\n",
    "  var_ratio.append(np.sum(pca.explained_variance_ratio_))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(4,2),dpi=150)\n",
    "plt.grid()\n",
    "plt.plot(nums,var_ratio,marker='o')\n",
    "plt.xlabel('n_components')\n",
    "plt.ylabel('Explained variance ratio')\n",
    "plt.title('n_components vs. Explained Variance Ratio')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "cmap = plt.get_cmap(\"tab20\")\n",
    "\n",
    "clusters = pca_df[\"Cluster\"].to_list()\n",
    "\n",
    "# Plot each sample category individually such that we can set label name.\n",
    "for i, clusterId in enumerate(clusters):\n",
    "    sub_matrix = np.array(pca_df[pca_df[\"Cluster\"] == clusterId][\"embed_vis\"].to_list())\n",
    "    \n",
    "    x=sub_matrix[:, 0]\n",
    "    y=sub_matrix[:, 1]\n",
    "    z=sub_matrix[:, 2]\n",
    "    colors = [cmap(i/len(clusters))] * len(sub_matrix)\n",
    "    ax.scatter(x, y, zs=z, zdir='z', c=colors, label=clusterId)\n",
    "\n",
    "    students = pca_df[pca_df[\"Cluster\"] == clusterId].index.values.tolist()\n",
    "    for i, txt in enumerate(students):\n",
    "        ax.text(x[i], y[i], z[i], txt, size=8, zorder=1, color='k')\n",
    "\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_zlabel('z')\n",
    "# ax.legend(bbox_to_anchor=(1.1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
